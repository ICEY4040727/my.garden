---
{"dg-publish":true,"permalink":"/llm//","created":"2025-12-20T02:02:25.262+08:00","updated":"2025-12-25T16:11:05.518+08:00"}
---


# LLM全景图

	LLM通常基于Transformer Decoder架构，主要采用Decoder-Only和Mixture of Expert(MoE)。
	主要区别为MoE在前馈网络部分引入了多个专家网络
	多模态LLM(MLLM)和视觉LLM(VLM)便是以LLM的强大知识能力为核心



# LLM结构全景图

	可分为三部分：
	输入层、多层Decoder、输出层（语言模型头、解码模块）



![QQ_1766170275892.png|50%](/img/user/QQ_1766170275892.png)
## 一个典型的llm结构

### 输入层

将输入文本转化为词元（token）序列。每个词元进一步映射到对应的Token ID，并最终转换为 **多维数值矩阵**。这些矩阵作为初始输入，传递给多层Decoder堆叠结构进行计算。

### 多层Decoder堆叠结构
	llm的核心，油多个相同的Decoder层堆叠而成，通过逐层计算，模型逐步总结和建模输入序列的深层语义与依赖关系。

#### 1.自注意力机制（Self-Attention）

##### 概念

通过 **(Q)查询(K)键(V)值** 向量进行注意力计算，并结合**因果掩码（Causal Masking）**，保证每个词元只能关注其之前的序列位置。这种机制使得模型能够建模序列中不同词元间的语义关系。

##### 形式

- 多头注意力（MHA）
- 多查询注意力(MQA)
- 分组查询注意力(GMA)
- 多头潜在注意力(MLA)
- 原生稀疏注意力(NSA)
- 混合分块注意力(MoBA)
- 线性注意力(例如Lighting Attention等)

#### 位置编码（Positional Encoding）

##### 概念

通过为序列中的词元引入位置信息，帮助模型理解序列的相对和绝对位置

##### 形式

- RoPE方法


#### 前馈网络（Feed-Forward Network，FFN）

##### 概念

对经过自注意力机制计算的隐藏状态（）进行非线性变换





